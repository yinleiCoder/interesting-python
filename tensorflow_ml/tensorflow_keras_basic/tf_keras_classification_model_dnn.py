# -*- coding: utf-8 -*-
"""tf_keras_classification_model_dnn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BZBvG8-TD7lrmgwRr6Eu7j93hGyvaylT

## 神经网络

- 神经网络正向计算
- 神经网络训练：梯度下降(求导、更新参数)

## 深度神经网络

- 层次非常深的神经网络

## 激活函数

- Sigmoid
- Leaky ReLU
- tanh 
- Maxout
- ReLU
- ELU

## 归一化与批归一化

- 归一化
  - Min-max归一化：x*=(x-min)/(max-min)
  - Z-score归一化：x*=(x-u)/方差
- 批归一化
  - 每层的激活值都做归一化

## Dropout

- 防止过拟合
  - 过拟合：训练集上很好，测试集上不好
  - 参数太多，记住样本，不能泛化
"""

import matplotlib as mpl 
import matplotlib.pyplot as plt 
import numpy as np 
import sklearn
import pandas as pd 
import os 
import sys 
import time 
import tensorflow as tf
from tensorflow import keras

print(tf.__version__)
print(sys.version_info)
for module in mpl, np, pd, sklearn, tf, keras:
  print(module.__name__, module.__version__)

fashion_mnist = keras.datasets.fashion_mnist
(x_train_all, y_train_all), (x_test, y_test) = fashion_mnist.load_data()
x_valid, x_train = x_train_all[:5000], x_train_all[5000:]
y_valid, y_train = y_train_all[:5000],  y_train_all[5000:]

print(x_valid.shape, y_valid.shape)
print(x_train.shape, y_train.shape)
print(x_test.shape, y_test.shape)

# 数据归一化：x=(x-u)/std
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
# x_train: [None, 28, 28] -> [None, 784]
x_train_scaled = scaler.fit_transform(
    x_train.astype(np.float32).reshape(-1, 1)).reshape(-1, 28, 28)
x_valid_scaled = scaler.transform(
    x_valid.astype(np.float32).reshape(-1, 1)).reshape(-1, 28, 28)
x_test_scaled = scaler.transform(
    x_test.astype(np.float32).reshape(-1, 1)).reshape(-1, 28, 28)

print(np.max(x_train_scaled), np.min(x_train_scaled))

model = keras.models.Sequential()
model.add(keras.layers.Flatten(input_shape=[28, 28]))
# 深度神经网络
for _ in range(20):
  # selu是自带归一化的激活函数，相对于BatchNormalization更好
  model.add(keras.layers.Dense(100, activation='selu'))
  # 批归一化
  # model.add(keras.layers.BatchNormalization())
# Dropout: AlphaDropout均值和方差不变、归一化性质不变
model.add(keras.layers.AlphaDropout(rate=0.5))
model.add(keras.layers.Dense(10, activation='softmax'))

# relu: y=max(0, x)
# softmax: 将向量变为概率分布 x=[x1, x2, x3]
#      y=[e^x1/sum, e^x2/sum, e^x3/sum], sum=e^x1+e^x2+e^x3

# reason for sparse: y->index, y->one_hot->[]
model.compile(loss='sparse_categorical_crossentropy',
        optimizer='sgd',
        metrics=['accuracy'])

model.summary()

# 回调函数Tensorboard, earlystopping, ModelCheckpoint
logdir = './dnn-bn-selu_dropout-callbacks'
if not os.path.exists(logdir):
  os.mkdir(logdir)
output_model_file = os.path.join(logdir, 'fashion_mnist_model.h5')
callbacks = [
    keras.callbacks.TensorBoard(logdir),
    keras.callbacks.ModelCheckpoint(output_model_file,
                    save_best_only=True),
    keras.callbacks.EarlyStopping(patience=5, min_delta=1e-3)
]

history = model.fit(x_train_scaled, y_train, epochs=50,
      validation_data=(x_valid_scaled, y_valid),
      callbacks=callbacks)

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir dnn-bn-selu_dropout-callbacks

def plot_learning_curves(history):
  pd.DataFrame(history.history).plot(figsize=(8, 5))
  plt.grid(True)
  plt.gca().set_ylim(0, 3)
  plt.show()

plot_learning_curves(history)
# 1.参数众多，训练不充分
# 2.梯度消失 -> 链式法则 -> 复合函数求导f(g(x))
#   批归一化缓解梯度消失
#   selu激活函数自带归一化，缓解梯度消失

model.evaluate(x_test_scaled, y_test)