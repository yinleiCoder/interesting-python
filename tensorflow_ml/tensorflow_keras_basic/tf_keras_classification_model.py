# -*- coding: utf-8 -*-
"""tf_keras_classification_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Q2ozi7smj1cYDUfaezaqrS5dd7tSeFGy

# **Kears**

- 基于Python的高级神经网络API
- Francois Chollet于2014-2015年编写Kears
- 以Tensorflow、CNTK或Theano为后端运行，keras必须有后端才可以运行
- 后端可以切换，现在多用Tensorflow
- 极方便于快速实验，帮助用户以最少的时间验证自己的想法

# **Tensorflow-keras**

- Tensorflow对keras API规范的实现
- 相对于以Tensorflow为后端的keras，Tensorflow-keras与Tensorflow结合的更加紧密
- 实现在tf.keras空间下

### Tensorflow和keras

- 基于同一套API。keras程序可以通过改导入方式轻松转为tf.keras，反之不成立，因为tf.keras有其他特性
- 相同的JSON、HDF5模型序列化格式和语义
- tf.keras全面支持eager mode
  - 只是使用keras.Sequential、keras.Model时没有影响
  - 自定义Model内部运算逻辑的时候会有影响
    - tf底层API可以使用keras的model.fit等抽象
    - 更适用于研究人员
- tf.keras支持基于tf.data的模型训练
- tf.keras支持TPU训练
- tf.keras支持tf.distribution的分布式策略
- tf.keras可以与tensorflow中的estimator集成
- tf.keras可以保存为SavedModel

# **分类问题与回归问题**

- 分类问题预测的是类别，模型的输出是概率分布
  - 如：三分类问题输出例子：[0.2, 0.7, 0.1]
- 回归问题预测的是值，模型的输出是一个实数值
  - 如：房价预测

## 目标函数

- 参数是逐步调整、逼近的
- 目标函数可以帮助衡量模型的好坏
  - 如：Model A: [0.1, 0.4, 0.5]
  -   Model B: [0.1, 0.2, 0.7]
- 对于分类问题
  - 需要衡量目标类别与当前预测的差距
    - 三分类问题输出例子：[0.2, 0.7, 0.1]
    - 三分类真实类别：2 -> one_hot -> [0, 0, 1]
  - 常用损失函数：平方差损失、交叉熵损失
- 对于回归问题
  - 预测值与真实值的差距
  - 常用损失函数：平方差损失、绝对值损失
- 机器学习的大部分模型的训练就是调整参数，使得目标函数逐渐变小的过程

## One-hot编码
把正整数变为向量表达，生成一个长度不小于正整数的向量，只有正整数的位置处为1，其余位置都为0
"""

import matplotlib as mpl 
import matplotlib.pyplot as plt 
import numpy as np 
import sklearn
import pandas as pd 
import os 
import sys 
import time 
import tensorflow as tf
from tensorflow import keras

print(tf.__version__)
print(sys.version_info)
for module in mpl, np, pd, sklearn, tf, keras:
  print(module.__name__, module.__version__)

fashion_mnist = keras.datasets.fashion_mnist
(x_train_all, y_train_all), (x_test, y_test) = fashion_mnist.load_data()
x_valid, x_train = x_train_all[:5000], x_train_all[5000:]
y_valid, y_train = y_train_all[:5000],  y_train_all[5000:]

print(x_valid.shape, y_valid.shape)
print(x_train.shape, y_train.shape)
print(x_test.shape, y_test.shape)

# 数据归一化：x=(x-u)/std
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
# x_train: [None, 28, 28] -> [None, 784]
x_train_scaled = scaler.fit_transform(
    x_train.astype(np.float32).reshape(-1, 1)).reshape(-1, 28, 28)
x_valid_scaled = scaler.transform(
    x_valid.astype(np.float32).reshape(-1, 1)).reshape(-1, 28, 28)
x_test_scaled = scaler.transform(
    x_test.astype(np.float32).reshape(-1, 1)).reshape(-1, 28, 28)

print(np.max(x_train_scaled), np.min(x_train_scaled))

def show_single_image(img_arr):
  plt.imshow(img_arr, cmap="binary")
  plt.show()

show_single_image(x_train[0])

def show_imgs(n_rows, n_cols, x_data, y_data, class_names):
  assert len(x_data) == len(y_data)
  assert n_rows * n_cols < len(x_data)
  plt.figure(figsize=(n_cols*1.4, n_rows*1.6))
  for row in range(n_rows):
    for col in range(n_cols):
      index = n_cols * row + col
      plt.subplot(n_rows, n_cols, index+1)
      plt.imshow(x_data[index], cmap="binary", interpolation='nearest')
      plt.axis('off')
      plt.title(class_names[y_data[index]])
  plt.show()

class_names = ['T-shirt', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']
show_imgs(3, 5, x_train, y_train, class_names)

model = keras.models.Sequential()
model.add(keras.layers.Flatten(input_shape=[28, 28]))
model.add(keras.layers.Dense(300, activation='relu'))
model.add(keras.layers.Dense(100, activation='relu'))
model.add(keras.layers.Dense(10, activation='softmax'))
# relu: y=max(0, x)
# softmax: 将向量变为概率分布 x=[x1, x2, x3]
#      y=[e^x1/sum, e^x2/sum, e^x3/sum], sum=e^x1+e^x2+e^x3

# reason for sparse: y->index, y->one_hot->[]
model.compile(loss='sparse_categorical_crossentropy',
        optimizer='sgd',
        metrics=['accuracy'])

model.layers

model.summary()

# [None, 784]*w + b -> [None, 300]
# w.shape[784, 300], b=[300]

# 回调函数Tensorboard, earlystopping, ModelCheckpoint
logdir = './callbacks'
if not os.path.exists(logdir):
  os.mkdir(logdir)
output_model_file = os.path.join(logdir, 'fashion_mnist_model.h5')
callbacks = [
    keras.callbacks.TensorBoard(logdir),
    keras.callbacks.ModelCheckpoint(output_model_file,
                    save_best_only=True),
    keras.callbacks.EarlyStopping(patience=5, min_delta=1e-3)
]

history = model.fit(x_train_scaled, y_train, epochs=50,
      validation_data=(x_valid_scaled, y_valid),
      callbacks=callbacks)

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir callbacks

def plot_learning_curves(history):
  pd.DataFrame(history.history).plot(figsize=(8, 5))
  plt.grid(True)
  plt.gca().set_ylim(0, 1)
  plt.show()

plot_learning_curves(history)

model.evaluate(x_test_scaled, y_test)